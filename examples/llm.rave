import <std/io> <std/math>

void encoderForward(float* out, int* inp, float* wte, float* wpe, int B, int T, int C) {
    // out is (B,T,C). At each position (b,t), a C-dimensional vector summarizing token & position
    // inp is (B,T) of integers, holding the token ids at each (b,t) position
    // wte is (V,C) of token embeddings, short for "weight token embeddings"
    // wpe is (maxT,C) of position embeddings, short for "weight positional embedding"
    for(int b=0; b<B; b++) {
        for(int t=0; t<T; t++) {
            // seek to the output position in out[b,t,:]
            float* out_bt = itop(float*, ptoi(out_bt) + b * T * C + t * C);
            // get the index of the token at inp[b, t]
            int ix = inp[b * T + t];
            // seek to the position in wte corresponding to the token
            float* wte_ix = itop(float*, ptoi(wte) + ix * C);
            // seek to the position in wpe corresponding to the position
            float* wpe_t = itop(float*, ptoi(wpe) + t * C);
            // add the two vectors and store the result in out[b,t,:]
            for(int i=0; i<C; i++) out_bt[i] = wte_ix[i] + wpe_t[i];
        }
    }
}

void encoderBackward(float* dwte, float* dwpe, float* dout, int* inp, int B, int T, int C) {
    for(int b=0; b<B; b++) {
        for(int t=0; t<T; t++) {
            float* dout_bt = itop(float*, ptoi(dout) + b * T * C + t * C);
            int ix = inp[b * T + t];
            float* dwte_ix = itop(float*, ptoi(dwte) + ix * C);
            float* dwpe_t = itop(float*, ptoi(dwpe) + t * C);
            for(int i=0; i<C; i++) {
                float d = dout_bt[i];
                dwte_ix[i] += d;
                dwpe_t[i] += d;
            }
        }
    }
}

void layernormForward(float* out, float* mean, float* rstd, float* inp, float* weight, float* bias, int B, int T, int C) {
    // reference: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html
    // both inp and out are (B,T,C) of the activations
    // mean and rstd are (B,T) buffers, to be used later in backward pass
    // at each position (b,t) of the input, the C-dimensional vector
    // of activations gets normalized, then scaled and shifted
    float eps = cast(float)0.00001;
    for(int b=0; b<B; b++) {
        for(int t=0; t<T; t++) {
            // seek to the input position inp[b,t,:]
            float* x = itop(float*, ptoi(inp) + b * T * C + t * C);
            // calculate the mean
            float m = cast(float)0.0;
            for(int i=0; i<C; i++) m += x[i];
            m = m/C;
            // calculate the variance (without any bias correction)
            float v = cast(float)0.0;
            for(int i=0; i<C; i++) {
                float xshift = x[i] - m;
                v += xshift * xshift;
            }
            v = v/C;
            // calculate the rstd (reciprocal standard deviation)
            float s = cast(float)1.0 / cast(float)std::math::sqrt(cast(double)(v + eps));
            // seek to the output position in out[b,t,:]
            float* out_bt = itop(float*, ptoi(out) + b * T * C + t * C);
            for(int i=0; i<C; i++) {
                float n = (s * (x[i] - m)); // normalize
                float o = n * weight[i] + bias[i]; // scale and shift
                out_bt[i] = o; // write
            }
            // cache the mean and rstd for the backward pass later
            mean[b * T + t] = m;
            rstd[b * T + t] = s;
        }
    }
}

void layernormBackward(float* dinp, float* dweight, float* dbias, float* dout, float* inp, float* weight, float* mean, float* rstd, int B, int T, int C) {
    for(int b=0; b<B; b++) {
        for(int t=0; t<T; t++) {
            float* dout_bt = itop(float*, ptoi(dout) + b * T * C + t * C);
            float* inp_bt = itop(float*, ptoi(inp) + b * T * C + t * C);
            float* dinp_bt = itop(float*, ptoi(dinp) + b * T * C + t * C);
            float mean_bt = mean[b * T + t];
            float rstd_bt = rstd[b * T + t];

            // first: two reduce operations
            float dnorm_mean = cast(float)0.0;
            float dnorm_norm_mean = cast(float)0.0;
            for(int i=0; i<C; i++) {
                float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;
                float dnorm_i = weight[i] * dout_bt[i];
                dnorm_mean += dnorm_i;
                dnorm_norm_mean += dnorm_i * norm_bti;
            }
            dnorm_mean = dnorm_mean / C;
            dnorm_norm_mean = dnorm_norm_mean / C;

            // now iterate again and accumulate all the gradients
            for(int i=0; i<C; i++) {
                float norm_bti = (inp_bt[i] - mean_bt) * rstd_bt;
                float dnorm_i = weight[i] * dout_bt[i];
                // gradient contribution to bias
                dbias[i] += dout_bt[i];
                // gradient contribution to weight
                dweight[i] += norm_bti * dout_bt[i];
                // gradient contribution to input
                float dval = cast(float)0.0;
                dval += dnorm_i; // term 1
                dval -= dnorm_mean; // term 2
                dval -= norm_bti * dnorm_norm_mean; // term 3
                dval *= rstd_bt; // final scale
                dinp_bt[i] += dval;
            }
        }
    }
}

void matmulForward(float* out, float* inp, float* weight, float* bias, int B, int T, int C, int OC) {
    // most of the running time is spent here and in matmul_backward
    // OC is short for "output channels"
    // inp is (B,T,C), weight is (OC, C), bias is (OC)
    // out will be (B,T,OC)

    for(int b=0; b<B;b++) {
        for(int t=0; t<T; t++) {
            float* out_bt = itop(float*, ptoi(out) + b * T * OC + t * OC);
            float* inp_bt = itop(float*, ptoi(inp) + b * T * C + t * C);
            for(int o = 0; o < OC; o++) {
                float val;
                if(bias != cast(float*)null) val = bias[o];

                float* wrow = itop(float*, ptoi(weight) + o*C);
                for(int i = 0; i < C; i++) val += inp_bt[i] * wrow[i];
                out_bt[o] = val;
            }
        }
    }
}

void matmulBackward(float* dinp, float* dweight, float* dbias, float* dout, float* inp, float* weight, int B, int T, int C, int OC) {
    // most of the running time is spent here and in matmul_forward
    // this backward could be done in a single "round" of loops
    // but that doesn't afford an efficient parallelization strategy

    // backward into inp first, parallelize over B,T

    for (int b=0; b<B; b++) {
        for(int t=0; t<T; t++) {
            float* dout_bt = itop(float*, ptoi(dout) + b * T * OC + t * OC);
            float* dinp_bt = itop(float*, ptoi(dinp) + b * T * C + t * C);
            for(int o=0; o<OC; o++) {
                float* wrow = itop(float*, ptoi(weight) + o*C);
                float d = dout_bt[o];
                for(int i = 0; i < C; i++) dinp_bt[i] += wrow[i] * d;
            }
        }
    }
    // backward into weight/bias, parallelize over output channels OC

    for(int o=0; o<OC; o++) {
        for(int b=0; b<B; b++) {
            for(int t=0; t<T; t++) {
                float* dout_bt = itop(float*, ptoi(dout) + b * T * OC + t * OC);
                float* inp_bt = itop(float*, ptoi(inp) + b * T * C + t * C);
                float* dwrow = itop(float*, ptoi(dweight) + o*C);
                float d = dout_bt[o];
                if(dbias != cast(float*)null) dbias[o] += d;
                for(int i = 0; i < C; i++) dwrow[i] += inp_bt[i] * d;
            }
        }
    }
}

// TODO

void main {

}